{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from classifier import *\n",
    "from preprocessing import *\n",
    "from staff_removal import *\n",
    "from helper_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths of scaned and captured test cases #\n",
    "scanned_path = './test-cases/test-set-scanned/scanned'\n",
    "captured_path = './test-cases/test-set-camera-captured/captured'\n",
    "\n",
    "img_filenames = os.listdir(scanned_path)\n",
    "for i, fn in enumerate(img_filenames):\n",
    "    test_number = fn.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img_path, output_path):\n",
    "    # 1. Read desired image #\n",
    "    img = cv2.imread(img_path, 0)\n",
    "    \n",
    "    # 2. Remove noise (odd pixels) from the image and save it #\n",
    "    img = cv2.fastNlMeansDenoising(img, None, 10, 7, 21)\n",
    "    cv2.imwrite('testing-output/{}/1. noise_removed.png'.format(output_path), img)\n",
    "\n",
    "    # 3. Binarize image using combination of (global + otsu) thresholding and save it #\n",
    "    threshold, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    cv2.imwrite('testing-output/{}/2. binarized.png'.format(output_path), img)\n",
    "\n",
    "    # 4. Return image shape (width, height) and processed image # \n",
    "    n, m = img.shape\n",
    "    return n, m, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps of processing\n",
    "1. ~Apply Pre-processing~\n",
    "2. ~Remove Staff lines~\n",
    "3. ~Cut images into buckets~\n",
    "4. ~Get reference line for each bucket~\n",
    "5. ~Segment symbols, Sort them by x-value~\n",
    "6. Classify symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "finished processing:  a_1\n",
      "finished processing:  a_16\n",
      "finished processing:  a_16_flipped\n",
      "finished processing:  a_2\n",
      "finished processing:  a_2_flipped\n",
      "finished processing:  a_32\n",
      "finished processing:  a_4\n",
      "finished processing:  a_4_flipped\n",
      "finished processing:  a_8\n",
      "finished processing:  a_8_flipped\n",
      "finished processing:  b\n",
      "finished processing:  barline\n",
      "finished processing:  b_16\n",
      "finished processing:  b_16_flipped\n",
      "finished processing:  b_8\n",
      "finished processing:  b_8_flipped\n",
      "finished processing:  chord_2\n",
      "finished processing:  chord_3\n",
      "finished processing:  chord_3_2\n",
      "finished processing:  chord_special\n",
      "finished processing:  Clef\n",
      "finished processing:  d\n",
      "finished processing:  dot\n",
      "finished processing:  hash\n",
      "finished processing:  symbol_bb\n",
      "finished processing:  t_2\n",
      "finished processing:  t_4\n",
      "finished processing:  x\n",
      "Finished loading dataset.\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset. This will take time ...')\n",
    "features, labels = load_dataset('hog')\n",
    "print('Finished loading dataset.')\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Training SVM ##############\n",
      "SVM accuracy: 99.40334128878283 %\n"
     ]
    }
   ],
   "source": [
    "model_name = 'SVM'\n",
    "model = run_experiment(train_features, test_features, train_labels, test_labels, model_name)\n",
    "\n",
    "filename = f'{model_name}.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# TODO: Use this line to load the model \n",
    "# model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './data-set/a_4'\n",
    "\n",
    "# img_filenames = os.listdir(path)\n",
    "# for i, fn in enumerate(img_filenames):\n",
    "#     if fn != '':\n",
    "#         img = cv2.imread(f'{path}/{fn}', 0)\n",
    "#         img = clean_and_cut(img)\n",
    "#         cv2.imwrite(f'{path}/{fn}', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Threshold for line to be considered as an initial staff line #\n",
    "cnt = 0\n",
    "threshold = 0.8\n",
    "correct_labels = np.load('correct_labels.npy')\n",
    "for i in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10']:\n",
    "    try: \n",
    "        os.mkdir('testing-output/{}'.format(i)) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    # Get image and its dimensions#\n",
    "    height, width, in_img = preprocess_img('{}/{}.png'.format(scanned_path, i), '{}'.format(i))\n",
    "    \n",
    "    # Get line thinkness and list of staff lines #\n",
    "    staff_lines_thicknesses, staff_lines = get_staff_lines(width, height, in_img, threshold)\n",
    "\n",
    "#     TODO: remove after finishing classification\n",
    "#     print('test image {}'.format(i))\n",
    "#     print('staff_lines: ', staff_lines)\n",
    "#     print('staff_lines_thicknesses: ', staff_lines_thicknesses, end='\\n\\n')\n",
    "\n",
    "    # Remove staff lines from original image #\n",
    "    cleaned = remove_staff_lines(in_img, width, staff_lines, staff_lines_thicknesses)\n",
    "    cv2.imwrite('testing-output/{}/3. cleaned.png'.format(i), cleaned)\n",
    "    \n",
    "    # Get list of cutted buckets and cutting positions #\n",
    "    cut_positions, cutted = cut_image_into_buckets(cleaned, staff_lines)\n",
    "    \n",
    "    \n",
    "    # Get reference line for each bucket #\n",
    "    ref_lines = get_ref_lines(cut_positions, staff_lines)\n",
    "    \n",
    "    for it in range(len(cutted)):\n",
    "        cur_img = cutted[it].copy()\n",
    "        symbols_boundries = segmentation(cutted[it])\n",
    "        symbols_boundries.sort(key = lambda x: (x[0], x[1]))\n",
    "        \n",
    "        symbols = []\n",
    "        for boundry in symbols_boundries:\n",
    "            # Get the current symbol #\n",
    "            x1, y1, x2, y2 = boundry\n",
    "            cur_symbol = cutted[it][y1:y2+1, x1:x2+1]\n",
    "            \n",
    "            # Clean and cut #\n",
    "            cur_symbol = clean_and_cut(cur_symbol)\n",
    "            cur_symbol = 255 - cur_symbol\n",
    "\n",
    "            # Start prediction of the current symbol #\n",
    "            feature = extract_features(cur_symbol, 'hog')\n",
    "            label = model.predict([feature])\n",
    "            \n",
    "            if correct_labels[cnt] != label:    \n",
    "                print(cnt, label)\n",
    "            \n",
    "                plt.axis('off')\n",
    "                io.imshow(cur_symbol)\n",
    "                io.show()\n",
    "                \n",
    "            cnt+=1\n",
    "            \n",
    "            \n",
    "        cv2.imwrite(f'testing-output/{i}/4. cutted-{it + 1}.png', cutted[it])\n",
    "        cv2.imwrite(f'testing-output/{i}/5. cutted-segmented-{it + 1}.png', cur_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
